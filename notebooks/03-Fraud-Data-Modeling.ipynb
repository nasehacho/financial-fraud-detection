{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chosen Model: Random Forest Classifier** \n",
    "\n",
    "Random forest algorithm parameters:\n",
    "\n",
    "n_estimators: The number of trees in the forest.  \n",
    "\n",
    "criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  \n",
    " \n",
    "min_samples_split: The minimum number of samples required to split an internal node.  \n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n",
    "\n",
    "min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.  \n",
    "\n",
    "max_features: The number of features to consider when looking for the best split.  \n",
    "\n",
    "max_leaf_nodes: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.  \n",
    "\n",
    "min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.  \n",
    "\n",
    "min_impurity_split: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  \n",
    "\n",
    "bootstrap: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.  \n",
    "\n",
    "oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from IPython.display import Image\n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "\n",
    "fraud_df = pd.read_csv('../data/fraud_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 'RandomForestClassifier' \n",
    "\n",
    "rf_clf = _ _ _(n_estimators=100)\n",
    "\n",
    "# fit the randomforest object using the X_train & y_train data\n",
    "rf_clf.fit(_ _ _, _ _ _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a report on the random forest model\n",
    "\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train test splits\n",
    "\n",
    "# Create a 'X' and 'Y' training set, where 'X' is simply the dataframe without the 'Attrition' column\n",
    "X = df.drop('_ _ _', axis=1)\n",
    "\n",
    "# and 'Y' is the 'Attrition' column\n",
    "y = df['_ _ _']\n",
    "\n",
    "# create 'train_test_split' splits on the X and Y data where the test_size is 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(_, _, test_size=_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chosen Hyperparameter Tuning: GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the best possible hyperparameters, using RandomSearchCV\n",
    "# NOTE: This might take some time\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators, \n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth, \n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf, \n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "# TODO: Implement `RandomizedSearchCV` object\n",
    "rf_cv = _ _ _(\n",
    "    estimator=rf_clf, \n",
    "    scoring='f1',\n",
    "    param_distributions=random_grid, \n",
    "    n_iter=200, \n",
    "    cv=5, \n",
    "    verbose=1, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_cv.fit(X_train, y_train)\n",
    "rf_best_params = rf_cv.best_params_\n",
    "print(f\"Best paramters: {rf_best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(**rf_best_params)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-train model using optimal hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
